Red Hat OpenShift 4 Foundations Notes
====================================

Architecture
============

Topics:
- Overview
- Product and Infrastructure Providers
- Architecture and Concepts
- Operators
- Networking Workflow
- Container Deployment Workflow
	
Red Hat OpenShift Container Platform:
- Container orchestration platform based on Kubernetes
- Benefits both operations and development
- Provides developers and IT organizations with cloud application platform
	- Used for deploying applications on secure, scalable resources
	- Minimal configuration and management overhead
- Supports Java™, Python, Ruby, Node.js, Perl, PHP, .NET, and more
- Built on Kubernetes
- OpenShift Container Platform’s Control Plane is only available to be deployed on RHCOS
- OpenShift Container Platform workloads may be deployed on RHCOS or Red Hat Enterprise Linux® (RHEL)
	- RHCOS available only for OpenShift deployments, not for general use
	- RHCOS codifies operational expertise for OpenShift with new purpose-built tooling
- RHCOS is FIPS-compliant
- Brings Kubernetes platform to customer data centers and cloud
	- Meets security, privacy, compliance, and governance requirements

- OpenShift 4 is truly hybrid cloud solution

Supported Infrastructures:
- can be fully deployed on public and private clouds, as well as bare metal
- capable of automating infrastructure deployment and product deployment aka Installed Provisioned Infrastructure(IPI)
- Full stack automation (IPI) supported on:
	- AWS, Microsoft Azure, Google Cloud Platform
	- Red Hat Virtualization, Red Hat OpenStack Platform
- Preexisting infrastructure (UPI- User Provisioned Infrastructure) supported on:
	- AWS, Microsoft Azure, Google Cloud Platform
	- VMware vSphere, Red Hat OpenStack Platform, IBM Z
	- IBM Power Systems, bare metal
- Future releases to support even more infrastructures
- You can also install OpenShift on all of these infrastructure and federate them

Product and Infrastructure Providers:
OpenShift 3.11 is Supported anywhere RHEL runs
    - Bare-metal physical machines, virtualized infrastructure, in private or certified public clouds
    - Virtualization platforms: Red Hat Virtualization, vSphere, Hyper-V
    - Red Hat OpenStack Platform, certified public cloud providers like Amazon, Google, Azure
    - x86 and IBM Power server architectures
OpenShift Kubernetes Engine
    - Users explore OpenShift 4 Kubernetes Engine, not entire platform
    - Core Kubernetes functionality with big ISV ecosystem
    - Enjoy RHCOS immutable and secure architecture
    - Appeals to DIY, *KS, or lower end

Architecture and Concepts
-------------------------
Node Host
---------
- OpenShift runs on RHCOS and RHEL
- OpenShift has two types of nodes:
	- Workers
	- Masters
- Nodes are:
	- Instances of RHEL or RHCOS with OpenShift installed
	- Workers: Where end-user applications run
	- Masters: Manage the cluster. Its the Control Plane. Orchestrates worker nodes
- OpenShift node doemon software run on all nodes

Containers
----------
- Application instances and components run in OCI-compliant containers in worker nodes
- Container Image: Has everything required to run an application. Application binary, dependencies, runtime
- Container: Running instance of image
- A worker node can run many containers
- A worker node capacity related to memory and CPU capabilities of underlying resources wheather in Cloud or hardware, or virtualized

Pod
----
- A pod is a smallest compute unit that can be defined, deployed, managed
- A pod consist of one or more containers, which are tightly coupled, and share resources such as volumes and IP addresses
- A pod Models application-specific logical host in containerized environment
- In pre-container world, applications executed on same physical or virtual host
- In OpenShift, pods replace individual application containers as smallest deployable unit

- A pod is an Orchestrated unit in OpenShift
    - OpenShift schedules and runs all containers in pod on same node
- Complex applications made up of many pods, each with own containers
    - Interact externally and also with one another inside OpenShift environment
- Possible to have multiple containers in single pod
    - Example: To support cluster features as sidecar containers
- Most applications benefit from flexibility of single-container pod
    - Different components such as application server and database generally not placed in single pod
    - Allows for individual application components to be easily scaled horizontally
- Application components are wired together by services

- To view pods running in environment
	oc get pods

Service
-------
- Defines logical set of pods and access policy
    - Provides permanent internal IP address and host name for other applications to use as pods are created and destroyed
- Service layer connects application components together
    - Example: Front-end web service connects to database instance by communicating with its service
- Services allow simple internal load balancing across application components
    - OpenShift automatically injects service information into running containers for ease of discovery

Labels
------
- Used to organize, group, or select API objects
	- Example: Tag pods with labels, services use label selectors to identify pods they proxy to
	- Makes it possible for services to reference groups of pods
	- Able to treat pods with potentially different containers as related entities
- Most objects able to include labels in metadata
- Group arbitrarily related objects with labels
	- Example: All pods, services, replication controllers, and deployment configurations of application
- Labels are simple key-value pairs: e.g.
	Labels:
	  key1: value1
	  key2: value2
	  
Master Nodes
-----------
- Master nodes installs only on RHCOS. Cannot be deployed on RHEL
- Primary functions:
	- Orchestrate activities on worker nodes
	- Know and maintain state within OpenShift environment
- Three masters for high availability
- Some infrastructure services are unique to OpenShift
		
etcd
-----
- Desired and current state held in data store that uses etcd as distributed key-value store
- etcd also holds:
	- RBAC rules
	- Application environment information
	- Non-application user data

Master Services - Core Kubernetes Components
--------------
- OpenShift includes Kubernetes services
- Byte-for-byte identical to Cloud Native Computing Foundation’s Kubernetes
- Provides:
	- Kubernetes API server
	- Scheduler
	- Cluster management services
	
Master Services - Core OpenShift Components
---------------
- OpenShift features core OpenShift components
- OpenShift API server
	- API calls unique to OpenShift
- Operator Lifecycle Manager (OLM)
	- Over-the-air updates of Operators for workloads, middleware, etc.
- Web console

	 -----------------------
	| OpenShift Services	|	-	1.OpenShift API server  2.OLM 3.Web Console
	| Kubernetes Services	|	-	1.Kubernetes API server  2.Scheduler  3.Cluster management services
	| etcd 					|	-	1.key-value store that holds desired and current state 2. RBAC Rules 3. App env. info
	 -----------------------
	Master node

Master - Infrastructure Services
----------------
- Masters and workers collaborate
- Both OpenShift and Kubernetes services included
- Run as pods; orchestrated by master control plane
- Services include:
	- Monitoring
	- Logging
	- OS tuning
	- Software defined networking (SDN)
	- DNS
	- Kubelet (OpenShift node process)

									 ------ Other pods----------         ------ Other pods----------
	 -----------------------		|							|       |							|
	| OpenShift Services	|		|---------------------------|       |---------------------------|
	| Infrastructure Serv.	|-------|							|       |							|
	| Kubernetes Services	|		|----------Pod--------------|       |----------Pod--------------|
	| etcd 					|		| Monitoring, Logging etc.	|       | Monitoring, Logging etc.	|
	 -----------------------		 ---------------------------         ---------------------------
	Master node						Worker                              Worker
	
Master - API and Authentication
---------------
- Masters provide single API endpoint that all tooling and systems interact with
	- Includes OpenShift and Kubernetes
	- All administration requests go through this API
- All API requests SSL-encrypted and authenticated
- Authorizations handled via fine-grained role-based access control (RBAC)
- Masters able to be tied into external identity management systems
	- Examples: LDAP, Active Directory, OAuth providers like GitHub and Google

Access
-------
- All users access OpenShift through same standard interfaces
- Web UI, CLI, IDEs all go through authenticated, RBAC-controlled API
- Users do not need system-level access to OpenShift nodes
- CI/CD systems integrate with OpenShift through these interfaces
- OpenShift deployed on RHCOS enables use of next-generation systems management and monitoring tools
- OpenShift deployed on RHEL enables use of existing systems management and monitoring tools
	
Health and Scaling
------------------
- Masters monitor health of pods and automatically scale them
	- User configures pod probes for liveness and readiness
	- Pods may be configured to automatically scale based on CPU utilization metrics

Unhealthy Pods
--------------
- What happens when masters note pod failing its probes?
- What happens if containers inside pod exit because of crash or other issue?

Remediating Pod Failures
-----------------------
- Masters automatically restart pods that fail probes or exit due to container crash
- Pods that fail too often marked as bad and temporarily not restarted
- Service layer sends traffic only to healthy pods
	- Masters automatically orchestrate this to maintain component availability

Scheduler
---------
- Component responsible for determining pod placement
- Accounts for current memory, CPU, and other environment utilization when placing pods on worker nodes
- For application high availability, spreads pod replicas between worker nodes

Scheduler Configuration
-----------------------
- Able to use real-world topology for OpenShift deployment (regions, zones, etc.)
- Handles complex scenarios for scheduling workloads
- Uses configuration in combination with node groups and labels
-     Example: Use regions and zones to carve up OpenShift environment to look like real-world topology
- OpenShift 4 can be configured to schedule more infrastructure, automatically

Integrated Container Registry
------------------------
- OpenShift Container Platform includes integrated container registry to store and manage container images
- When new image pushed to registry, registry notifies OpenShift and passes along image information including:
	- Namespace
	- Name
	- Image metadata
- Various OpenShift components react to new image by creating new builds and deployments
	
Application Data
----------------
- Containers natively ephemeral
	- Data not saved when containers restarted or created
- OpenShift provides persistent storage subsystem that automatically connects real-world storage to correct pods
	- Allows use of stateful applications
- OpenShift Container Platform provides wide array of persistent storage types including:
	- Raw devices: iSCSI, Fibre Channel
	- Enterprise storage: NFS
	- Cloud-type options: Ceph®, AWS EBS, pDisk
	
Routing Layer
--------------
- Provides external clients access to applications running inside OpenShift
- Close partner to service layer
- Runs in pods inside OpenShift
- Provides:
	- Automated load balancing to pods for external clients
	- Load balancing and auto-routing around unhealthy pods
- Routing layer pluggable and extensible
	- Options include non-OpenShift software routers
	
ReplicaSet and Replication Controller
-------------------
- Two implementations:
	- ReplicaSet = Kubernetes
	- Replication controller = OpenShift
- Ensures specified number of pod replicas running at all times
- If pods exit or are deleted, replication controller instantiates more
- If more pods running than needed, ReplicaSet deletes as many as necessary 

Deployment and DeploymentConfig
------------------------
- Two implementations: User may select either
	- Deployment = Kubernetes
	- DeploymentConfig = OpenShift
- Deployment controls how applications rolled out as pods
	- take name of the container image to be taken from image registry and deploy it as pod on node
	- they set the number of replicas of pod to deploy
	- labes indicated instruct scheduler to decide on which node to deploy pod
- Define how to roll out new versions of pods
- Identify:
	- Image name
	- Number of replicas
	- Label target deployment nodes
- Update pods based on:
	- Version
	- Strategy
	- Change triggers
- Create ReplicaSets or replication controllers 
- DeploymentConfig add the additional feature of change triggers
	- able to automtically create new version of deployment as new version of container image is available or other changes.
- Deployment, DeploymentConfig, ReplicaSet and Replication Controller 
	- cannot perform autoscaling
	- cannot deploy based on loader traffic
	- nor do they track autoscaled pods
	
Kubernetes Operator
------------------
- An operator is a Kubernetes-native application
- Goal of an operator is to put operational knowledge into software
- Previously this knoledge was only in minds of administrator
- Various softwares like shell scripts and automation software was outside kubernetes and hard to integrate
	- With operator integration is now possible
- Operator is purpose built for your application such as MongoDB
	- They implement and automate day1 activities like installation and configuration
	- and day2 activity such as scaling up and down, reconfiguration, updates, backups, failovers and 
	- restore a piece of software running in kubernetes cluster by interating natively with kubernetes concepts and API
	
- Puts all operational knowledge into Kubernetes primitives
- Administrators, shell scripts, automation software (e.g. Ansible®) now in Kubernetes pods
- Integrates natively with Kubernetes concepts and APIs

- An operator is a piece of software running in a pod on kubernetes cluster interacting with Kubernetes API server
- that is how it gets notified about presence and modification of app objects

Operators:
- Are pods with operator code that interact with Kubernetes API server
- Run "reconciliation loops" to check on application service
-     Make sure user-specified state of objects is achieved
- Manage all deployed resources and your application
- Act as application-specific controllers
- Extend Kubernetes API with Custom Resource Definition (CRD) introduced in Kubernetes 1.7

- An operator for an application can introduce a new CRD e.g. ABC
	- This is treated like any other object in Kubernetes such as Service
	- Promise that an operator is a custom form of controller
- a controller is basically a software loop that runs continuously in master node
	- in loop control logic looks for certain objects of interest 
	- it checks the desired state of these object expressed by the user. Compares that with current state
		and then does everything in its power to bring current state to desired state

Kubernetes Operator Framework
------------------
- A toolkit to manage application instances on Kubernetes in an effective, automated and scalable way
		Automated Lifecycle Management
		Installation > Upgrade > Backup > Failure recovery > Metrics and insights > Tuning
Consist of 3 main parts-
- Operator SDK
	- Developers build, package, test operator
	- No knowledge of Kubernetes API complexities required
- Operator Lifecycle Manager (OLM)
	- Helps install, update, manage life cycle of all operators in cluster
- Operator Metering
	- Usage reporting for Operators and resources within Kubernetes

OperatorHub.io
--------------
- Kubernetes Internet community for sharing Operators
- Works for any Kubernetes environment
- Packages Operators for easy deployment and management
- Publicizes Operators and enables adoption
- Uses OLM to install, manage, update Operators 

Networking workflow
-------------------
OpenShift Networking
-------------------
- Provides Container networking based on integrated Open vSwitch
- provides Platform-wide routing tier to route traffic to applications
- Ability to plug in third-party SDN solutions
- Integrated with DNS and existing routing and load balancing

Route
------
- Exposes service by giving it externally reachable host name
- Consists of route name, service selector, and (optional) security configuration
- Router can consume defined route and endpoints identified by service
	- Provides named connectivity
	- Lets external clients reach OpenShift-hosted applications

Router
------
- Multi-tier applications easily deployed
	- Routing layer required to reach applications from outside OpenShift environment
- Router container can run on any node host in environment
	- Administrator creates wildcard DNS entry (CNAME or A record) on DNS server
	- DNS entry resolves to node host hosting router container
- Router is ingress point for traffic destined for OpenShift-hosted pods
	- Router container resolves external requests (https://myapp.cloudapps.openshift.opentlc.com)
		- Proxies requests to correct pods
- Router is different from Route resource
	Router is the load balancer and Route is the specific configuration

Scenario:
- External client points browser to myApp.apps.openshift.opentlc.com:80
    DNS resolves to host running router container
    Using openshift-sdn overlay network:
        Router checks if route exists for request
        Proxies request to internal pod IP:port (10.1.2.3:8080)

Pod Connectivity
---------------
- Pods use network of OpenShift node host to connect to other pods and external networks

Scenario:
- Pod transmits packet to pod in another node host in OpenShift environment
    Container sends packet to target pod using IP 10.1.2.3:8080
    OpenShift node uses Open vSwitch to route packet to OpenShift node hosting target container
    Receiving node routes packet to target container
		
Services and Pods
----------------
- Services often used to provide permanent IP address to group of similar pods
- Internally, when accessed, services load balance to appropriate backing pod
- Backing pods can be added to or removed from service arbitrarily while service remains consistently available
	- Enables anything that depends on service to refer to it at consistent internal address
- Services have DNS names internal to OpenShift
	- Example: my-service.my-project.svc.cluster.local
	- Pods have access to internal DNS

Scenario:
- Pod transmits packet to service representing one or more pods
    Container sends packet to target service using IP 172.30.0.99:9999
    When service requested, OpenShift node proxies packet to pod represented by service (10.1.2.3:8080)

Container Deployment Workflow
-----------------------------
Scenario:
New application requested via CLI, web console, or API
- OpenShift API/authentication:
    - Approves request, considering user’s permissions, resource quotas, and other information
    - Creates supporting resources as needed: deployment configuration, replication controllers, services, routes, persistent storage claims
- OpenShift scheduler:
    - Designates worker node for each pod, considering resources' availability and load, and application spread between nodes for application high availability
- OpenShift worker node:
    - Pulls down image to be used from external or integrated registry
    - Starts container (pod) on worker node


User Experience
===============

Topics:
- Web Console
- Users and Projects
- Quotas and Limits
- Logging and Monitoring
- Templates, Operators, and Helm 3

Web Console
-----------
- Provides GUI to perform Administrative, mgmt and toubleshooting tasks
- It supports both Administrator and Developer perspectives
- Runs as pods in master node as OpenShift Console project
- It is managed by an operator pod
- Customizable: ISV (Indepedent Software Vendors) can customize the console using operators and direct customizations
- Has Built-in metrics - No need for Grafana

Developer Perspective
------------------
- Offers several built-in ways to streamline the process of deploying application, services and databases
- Topology view
	- Application-centric
	- Shows components and status, routes, source code
	- Drag arrows to create relationships
	- Add components to applications easily
	
- Project Details section offers
	- Application Status, Resource Utilization, Project Events Streaming, Quota consumption
- Project Access
	- Control users and groups
- Metrics

Administrator Perspective
-----------------
- Overview
	- Status, Activity/Events, Inventory, Capacity, Utilization
- Every common resource type manageable
- Logging and metrics
- Advanced Settings to view: Updates, Operators, CRDs, role bindings, resource quotas

Users and Projects
------------------
Project
-------
- a project allows a community of users to organize and manage their content in isolation from other communities. 
- Projects are OpenShift extentions to Kubernetes namespaces, with additional features to enable user self-provisioning.
- different projects can have different user permissions and quotas attached to them.
 
- Allows groups of users or developers to work together
- Unit of isolation and collaboration
- Defines scope of resources
- Allows project administrators and collaborators to manage resources
- Restricts and tracks use of resources with quotas and limits

- A project is a Kubernetes namespace with additional annotations
	- Central vehicle for managing resource access for regular users
	- Lets community of users organize and manage content in isolation from other communities
- Users:
	- Receive access to projects from administrators
	- Have access to own projects if allowed to create them
- Each project has own:
    - Objects: Pods, services, replication controllers, etc.
    - Policies: Rules that specify which users can or cannot perform actions on objects
    - Constraints: Quotas for objects that can be limited
    - Service accounts: Users that act automatically with access to project objects

Users and User Types
--------------------
- Interactions with OpenShift® always associated with user
	- System permissions granted by adding roles to users or groups
- User types:
  Regular Users
	- How most interactive OpenShift users are represented
	- Created automatically in system upon first login, or via API
	- Represented with user object
  System Users
	- Many created automatically when infrastructure defined
	- Let infrastructure interact with API securely
	- Include: cluster administrator, per-node user, service accounts

Login and Authentication
------------------------
	- Every user must authenticate to access OpenShift
	- API requests lacking valid authentication are authenticated as anonymous user
	- Policy determines what user is authorized to do
- Web Console Authentication
	- Access web console at URL provided by your administrator
	- Provide login credentials to obtain token to make API calls
	- Use web console to navigate projects
- CLI Login
	- Use oc tool to log in to same address as web console
	- Provide login credentials to obtain token to make API calls
	 	-oc login -u <my-user-name> --server="<master-api-public-addr>:<master-public-port>"
	- Administrators can have key generated by cluster for password-less authentication

Quotas and Limits
-----------------
Resource Quotas
--------------
- OpenShift can limit:
	- Number of objects created in project
	- Amount of compute/memory/storage resources requested across objects in project
	- Based on specified label
	 	- Examples: To limit to department of developers or environment such as test
- Multiple teams can share single OpenShift cluster
	- Each team in own project or projects
	- Resource quotas prevent teams from depriving each other of cluster resources
- ResourceQuota object enumerates hard resource usage limits per project
- ClusterResourceQuota object enumerates hard resource usage limits for users across the cluster

LimitRanges
------------
- LimitRanges express CPU and memory requirements of pods' containers
	- Set request and limit of CPU and memory particular pods' container may consume
	- Aid OpenShift scheduler in assigning pods to nodes
- LimitRanges express quality of service tiers:
	- Best Effort
	- Burstable
	- Guaranteed
- Default LimitRange for all pods/containers can be set for each project

Compute Resources Managed by Quota Across Pods in Non-Terminal State
--------------------
Resource Name			Description
cpu	(requests.cpu)		Sum of CPU requests cannot exceed this value
memory (requests.memory) Sum of memory requests cannot exceed this value
limits.cpu				Sum of CPU limits cannot exceed this value
limits.memory			Sum of memory limits cannot exceed this value

- cpu and requests.cpu are the same value and can be used interchangeably. The same applies to memory and requests.memory

Object Counts Managed by Quota
---------------
- ResourceQuota can limit the total number of a particular type of object that may be created in a project

Resource Name			Description
pods					Total number of pods in non-terminal state that can exist in project 
						(pod is in terminal state if status.phase in (Failed, Succeeded) is true)
replicationcontrollers	Total number of replication controllers that can exist in project
resourcequotas			Total number of resource quotas that can exist in project
services				Total number of services that can exist in project
secrets					Total number of secrets that can exist in project
configmaps				Total number of ConfigMap objects that can exist in project
persistentvolumeclaims	Total number of persistent volume claims that can exist in project
openshift.io/imagestreams	Total number of image streams that can exist in project

Quota Enforcement
---------------
- After quota created in project:
	- Project restricts ability to create resources that may violate quota constraint
	- Usage statistics calculated every few seconds (configurable)
- If project modification exceeds quota:
	- Server denies action
	- Returns error message
- Error message includes:
	- Quota constraint violated
	- Current system usage statistics

Viewing Quota
-------------
- From web console, select:
	- demoproject project
	- Developer → More → Project Details
	- Scroll to see resource usage, availability
	- Based on requests and limits for CPU, memory
- Click RQ compute-resources:
	- Shows specific resource type quotas, usage reports
	- Shows combined container and pod requests, limits
- Alternatively, use CLI to view quota details:
    - Get list of quotas defined in demoproject project:
    $ oc get quota -n demoproject
    NAME                AGE
    besteffort          11m
    compute-resources   2m
    core-object-counts  29m
	
    - Describe core-object counts quota in demoproject project:	
	$ oc describe quota core-object-counts -n demoproject
	Name:			core-object-counts
	Namespace:		demoproject
	Resource		Used	Hard
	--------		----	----
	configmaps		3	10
	persistentvolumeclaims	0	4
	replicationcontrollers	3	20
	secrets			9	10
	services		2	10

LimitRanges
-----------
- define CPU and memory requirements of pods and containers
- Defaults for containers:
	kind: LimitRange
	apiVersion: v1
	metadata:
	name: test-core-resource-limits
	namespace: test
	spec:
	limits:
		- type: Container
		max:
			memory: 6Gi
		min:
			memory: 10Mi
		default:
			cpu: 500m
			memory: 1536Mi
		defaultRequest:
			cpu: 50m
			memory: 256Mi
		- type: Pod
		max:
			memory: 12Gi
		min:
			memory: 6Mi

Logging and Monitoring
----------------------
Container Log Aggregation
-------------------------
- Using EFK stack, cluster administrators can aggregate logs for range of OpenShift services
	- Able to provide access for application developers to view them
- Modified version of EFK stack (ELK) can be found at:
	- https://www.elastic.co/videos/introduction-to-the-elk-stack
- EFK stack useful for viewing logs aggregated from hosts and applications
	- May come from multiple containers or even deleted pods

- Three components make up EFK logging stack:
-     Elasticsearch: Object store where all logs stored
-     Fluentd: Gathers logs from nodes, feeds them to Elasticsearch
-     Kibana: Web UI for Elasticsearch
- After EFK deployed, stack aggregates logs from all nodes and projects into Elasticsearch
-     Provides Kibana UI to view them
- Cluster administrators can view all logs
- Developers can view only logs for projects for which they have permission

Fluentd
-------
- Pulls logs from container file system and OpenShift services on host
- Sends them to respective Elasticsearch clusters that store aggregated log data
- Users and platform admins access respective Kibana UIs to see application’s or platform’s aggregated logs

Metrics Collection and Alerting with Prometheus - Features
-------------------------------
- OpenShift (OCP) comes with Pre-configured and self-updating monitoring stack based on Prometheus open source project
- Provides monitoring of cluster components
- Ships with set of alerts and dashboards
	- May also install and use Grafana dashboards
- One UI for OpenShift administrator to view cluster’s metrics from all containers, components
- Metrics used by horizontal pod autoscalers (HPAs) to determine when and how to scale
	- CPU and memory-based metrics viewable from OpenShift Container Platform web console

Metrics Collection and Alerting with Prometheus - Design
--------------------------------
Openshift-monitoring includes:
- Cluster Monitoring Operator (CMO)
	- Watches deployed monitoring components and resources; and keeps them up to date
- Prometheus Operator (PO)
	- Manages Prometheus and Alertmanager
	- Automatically generates monitoring target configuration based on Kubernetes label queries
- Alertmanager 
	- Handle alert sent by client application such as Prometheus server
	- It taskes care of de-duplicating, grouping, and routing them to correct receiver integration such as Email, PagerDuty
	- It also takes care of silencing alerts
- node-exporter
	- Agent deployed on all Nodes to collect metrics about the Node.
- kube-state-metrics
	- Agent Converts kubernetes objects to metrics consumable by Prometheus

Metrics Collection and Alerting with Prometheus - HPA
-----------------------------
- Prometheus enables customer to Configure horizontal pod autoscaling (HPA) based on any metric from Prometheus
	- Autoscale based on any cluster-level metrics from OpenShift
	- Autoscale based on any application metrics
- Limitations
	- Prometheus Adapter connects to single Prometheus instance (or via Kubernetes service)
	- Manual deployment and configuration of adapter
	- Cluster administrator needs to whitelist cluster metrics for HPA

Templates, Operators, and Helm 3
--------------------------------
- Templates:
	- Can only create resources
	- Cannot manage or delete resources
	- Not associated with pod
- Operators:
	- Create, manage, and delete resources
	- Implemented by operator pods
- Helm 3:
	- Package of templates
	- Focussed on How application packaged
	- How package installed
- A helm chart is an application packages that contains templetes for a set of resources that are necessary to 
	run the application such as deployment and service
	A templte uses variables that are substituted with values when manifest is created
	Chart includes a values file that describes how to configure the resources
	
											Helm Chart		Operator
Packaging									Y				Y
App Installation							Y               Y
App Updates (kubernetes manifests)			Y               Y
App Upgrade (data migration, adoption etc)                  Y
Backup and Recovery                                         Y
Workload and Log analysis                                   Y
Intelligent scaling                                         Y
Auto tuning                                                 Y

What Is a Template?
--------------------
- Describes set of objects that can be parameterized and processed to produce list of objects for OpenShift to create
- A template can be Processd to create anything you have permission to create within project
-     Examples: Service, build configuration, deployment configuration
- Can also define set of labels to apply to every object defined in template

What Are Templates For?
- Create instantly deployable applications for developers or customers
- Provide option to use preset variables or randomize values (like passwords)

Labels in Templates
-------------------
- Used to manage generated resources
- Apply to every resource generated from template
- Organize, group, or select objects and resources
- Resources and pods are "tagged" with labels
- Labels allow services and replication controllers to:
	- Determine pods they relate to
	- Reference groups of pods
	- Treat pods with different containers as similar entities

Parameters in Templates
-----------------------
- Share configuration values between different objects in template
- Values can be static or generated by template
- Templates let you define parameters that take on values
	- Value substituted wherever parameter is referenced
	- Can define references in any text field in object definition
- Example:
	- Set generate to expression to specify generated values
	- from specifies pattern for generating value using pseudo-regular expression syntax
	
	 parameters:
	   - name: PASSWORD
		 description: "The random user password"
		 generate: expression
		 from: "[a-zA-Z0-9]{12}"
		 
Creating Templates from Existing Objects
--------------------------
- Can export existing objects from project in template form
- Modify exported template by adding parameters and customizations
- To export objects from project in template form:
	$ oc export all --as-template=<template_name>

Operator Hub
------------
- Deploy and manage application in cluster with Operator
- Discover Operators from Kubernetes community and Red Hat® partners curated by RedHat
- Install Operators on clusters to provide optional add-ons and shared services to developers
- Once installed, Capabilities provided by Operator appear in Developer Catalog, providing self-service experience
- Add shared applications, services, or source-to-image builders to your project from the Developer Catalog
- Cluster administrators can install additional applications that show up in Developer catalog automatically

Helm Charts and Operators
---------------------
- Helm charts similar to templates
	- Collection of files that describe related set of Kubernetes resources
- Choose from many available Helm charts
- Use charts with Helm operator to deploy application easily
- Helm charts are not as robust as true operators

Operator Maturity Level
------------------
Phase1	Basic Install		Automated app provisioning and configuration mgmt
Phase2	Seamless Upgrades	Patch and minor version upgrades supported
Phase3	Full lifecycle		App lifecycle, storage lifecycle (backup, failure recovery)
Phase4	Deep insights		Metrics, alerts, log processing and workload analysis
Phase5	Auto pilot			Horizontal/Vertical scaling, auto config tuning, abnormal detection, scheduling tuning

Operator Interaction with OpenShift
--------------------
- Operators are pods that take advantage of custom resource definitions (CRDs)
- CRDs allow extension of Kubernetes/OpenShift API
	- API then knows new resources
- CRDs allow creation of custom resources (CRs)
- Operator watches for creation of CR
- CRs managed in same way as stock OpenShift objects
	create, get, describe, delete, etc.
	Example:
		oc get tomcats

Custom Resource Definition Example
--------------------------
	apiVersion: apiextensions.k8s.io/v1beta1
	kind: CustomResourceDefinition
	metadata:
	  name: tomcats.apache.org
	spec:
	  group: apache.org
	  names:
		kind: Tomcat
		listKind: TomcatList
		plural: tomcats
		singular: tomcat
		shortNames:
		- tc
	  scope: Namespaced
	  version: v1alpha1
  
- after this CRD is created oc understands
	oc get tomcats
	oc describe tc mytomcat-instace

Operators - Custom Resource (Application) Creation
--------------------------
- Custom Resources are simple definitions of resource you want Operator to create
- Running Operator watches for CR to be created in either:
	- Entire OpenShift cluster
	- Project that Operator is running in
- When CR created, operator receives event
- Operator then creates all OpenShift resources that make up application that the operator manages

Example: Custom Resource Creation
------------------------
To create CR instance:
- Create YAML file with definition of CR:
	apiVersion: apache.org/v1alpha1
	kind: Tomcat
	metadata:
	  name: mytomcat
	spec:
	  replicaCount: 2

- Create resource in OpenShift:
	oc create -f mytomcat.yaml

Operators - Custom Resource Management
-------------------------
- To manipulate and examine CR, use oc commands:
	oc get tomcats
	oc describe tomcat mytomcat
	oc scale tomcats --replicas=2 # only if Operator supports scaling

- Do not scale ReplicaSets, Deployment, StatefulSets directly
	- Use Operator to scale
	- Operator continues to watch created resources and sets them back to initial states
- To delete all created OpenShift API objects, delete CR:
	oc delete tomcat mytomcat
- Operator set ownership referece on all created OpenShift API objects. therefore
	deleting the CR results in a cascading deletion of all created objects

Helm Charts
----------
- When Helm chart installed:
	- Values in values file replaced in chart templates
	- Release produced as instance of chart running in Kubernetes cluster
- Install same chart multiple times to create many releases
- Helm v3
	- No server-side component (Tiller)
	- Helm CLI interacts with Kubernetes APIs


Resource Exploration Lab
========================

https://www.opentlc.com/account/		// reset passwd
https://labs.opentlc.com/				// OPENTLC lab portal

- Openshift Master Console ( web console )
https://console-openshift-console.apps.shared-na4.na4.openshift.opentlc.com

https://github.com/sclorg/nodejs-ex		// Import from git

- Resources selection 
Deployment- creates an application in plain Kubernetes style. 
Deployment Config- creates an OpenShift style application. 
Knative Service- creates a microservice.

Add to project
--------------
Select a way to create an application, component or service from one of the options
- from Git - Import code from your git repository to be built and deployed
- Container image - Deploy an existing image from an image registry or image stream tag
- from Catalog - Browse the catalog to discover, deploy and connect to services
- from Dockerfile - Import your Dockerfile from your git repo to be built & deployed
- YAML - Create resources from their YAML or JSON definitions
- Database - Browse the catalog to discover database services to add to your application

Overview panel
--------------
Details tab:
- Pod replica count can be managed here
- For serverless applications, the Pods are automatically scaled down to zero when idle and scaled up depending on the channel traffic.
- Other aspects of the Pod configuration available
	Name, Namespace, Labels, Annotations, Status
Resources tab:
- Resources created by the Deployment, and their status is indicated here.
- Pods status and logs
- Builds status and logs. start a new build if needed
- Services
- Routes - permit external access to the Pods. URL to access them is listed.
Monitoring tab:
- CPU Usage
- Memory Usage
- Bandwidth
- Events

Edit source code 
- This feature is available only when you create applications using the From Git, From Catalog, and the From Dockerfile options.

Topology view - to group applications and resources within an application group
List View - to see a list of all your applications 

Instantiate Template: (e.g. MongoDB)
- automatically populated template with details for the MongoDB service.
- following resources will be created:
	- DeploymentConfig
	- PersistentVolumeClaim
	- Secret
	- Service

- To add the MongoDB service to the existing application group, hold SHIFT, select the mongodb Pod and drag it to 
	the application group "nodejs-ex-app"; the MongoDB service is added to the existing application group.
- adding it to an application group automatically adds the required labels to the component.
	e.g. app.kubernetes.io/part-of: nodejs-ex-app
- To integrate the app and the database, the Node.js application and the MongoDB database need to be configured elsewhere

Monitoring page
----------
Dashboard - shows combined metrics for the project
Metrics - allows to create custom graphs of Prometheus Metrics
Events - view events reported as a stream. can be filtered

Deployments and Deployment Configs
--------------------------
- API objects that provide two similar, but different, methods for fine-grained management over common user applications
- Both describes the desired state of a particular component of the application as a Pod template.
- Deployment Configs involve one or more ReplicationControllers, which contain a point-in-time record of the state of 
	a Deployment Config as a Pod template. 
  Deployments involve one or more ReplicaSets, a successor of ReplicationControllers.
  e.g.
  Deployments - nodejs-ex
  Deployment Configs - mongodb

Quiz 3
=======
Question 1
Which of the following is a true statement about projects?
A. Each project has its own resources, policies, and constraints/quotas
B. Projects allow a team or group of users to organize and manage their content in isolation from others
C. The OpenShift administrator can create projects and assign administrator users for them
D. All of the above
Ans: D 
Question 2
Quotas can be used to limit the number of objects created in a project and the amount of compute/memory/storage resources requested across objects.
TRUE
FALSE
Ans: A
Question 3
CPU and memory-based metrics are viewable from the Red Hat OpenShift Container Platform web console and are available for use by Horizontal Pod Autoscalers (HPAs).
TRUE
FALSE
Ans: A
Question 4
The EFK stack is useful for viewing logs aggregated from hosts and applications, regardless of whether they come from multiple containers or deleted pods.
TRUE
FALSE
Ans: A
Question 5
Which of the following is a true statement about templates?
A. A template describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift
B. A template can be processed to create anything you have permission to create within a project
C. A template can define a set of labels to apply to every object defined in the template
D. All of the above
Ans: D
Question 6
In the EFK stack used for aggregated logging, what is the role of Fluentd?
A. An object store where all logs are stored
B. It gathers logs from nodes and feeds them to Elasticsearch
C. A web UI for Elasticsearch
D. All of the above
Ans: B
Question 7
In the EFK stack used for aggregated logging, what is the role of Kibana?
A. An object store where all logs are stored
B. It gathers logs from nodes and feeds them to Elasticsearch
C. A web UI for Elasticsearch
D. All of the above
Ans: C
Question 8
Template parameters can be which of the following?
A. User defined via command line or web console
B. Static and defined in the template
C. Generated by a regular expression when the template is processed
D. All of the above
Ans: D
Question 9
Which of the following is a true statement about OpenShift metrics collection facilities?
A. The OpenShift node exposes metrics that Prometheus collects and stores
B. CPU and memory-based metrics are viewable from the Red Hat OpenShift Container Platform web console and are available for use by Horizontal Pod Autoscalers (HPAs)
C. Prometheus Metrics is a metrics engine that stores data persistently
D. All of the above
Ans: D
Question 10
Which of the following is a FALSE statement about Operators?
A. They extend the OpenShift API
B. They encode operational knowledge
C. They replace templates
D. They can be written in Ansible
Ans: C
Question 11
Which of the following is a true statement about the web console?
A. The Developer perspective has a robust topology view/tool
B. The web console runs as pods on the Master servers
C. You can upgrade the cluster from the Administrator perspective
D. All of the above
Ans: D
Question 12
Helm 3 support does away with server-side components and includes the Helm CLI.
TRUE
FALSE 
Ans: A